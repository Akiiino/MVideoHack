{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставленную задачу будем интерпретировать как задачу регрессии, так как в данных имелись дробные оценки. Среди использованных признаков лучше себя на этапе экспериментирования показали сведения из комментариев пользователя, поэтому будем использовать их, представляя каждое слово в виде вещественного вектора определённой размерности.\n",
    "\n",
    "Для решения используем нейронные сети — свёрточные и рекуррентные, так как они обычно показывают себя лучше в задаче обработки естественного языка. Обе архитектуры показывают достаточно хорошие результаты, но в силу того, что обучатся они всё же достаточно по-разному и на по-разному предобработанных данных, вместе они показывают результат лучше каждой в отдельности. Мы также экспериментировали с использованием более традиционных методов типа градиентного бустинга, но он давал результаты заметно хуже полученных из нейросетей.\n",
    "\n",
    "Сами архитектуры не очень сложные, но мы используем множественные входы для того чтобы лучше учитывать разницу между тем, что обычно пишут в трёх различных полях. Итоговая активация — sigmoid — принимает значения в диапазоне (0, 1), так что предсказания сети переводятся в реальные как (x * 4) + 1. Это было сделано для того, чтобы предсказанный рейтинг товара всегда находился в допустимом диапазоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Embedding, Input, Concatenate,LSTM,\\\n",
    "                            Bidirectional, AlphaDropout, Masking\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_split(filename):\n",
    "    df=pd.read_csv(filename).fillna(\"\")\n",
    "    train,test=train_test_split(df,test_size=0.2)\n",
    "    return train,test\n",
    "\n",
    "def clean(x):\n",
    "    return ''.join(m.lemmatize(re.sub('([^а-яa-z]+)',' ',x.lower()))).strip()\n",
    "\n",
    "def prepare_for_rnn(train,test):\n",
    "    train.comment=train.comment.apply(clean)\n",
    "    train.commentNegative=train.commentNegative.apply(clean)\n",
    "    train.commentPositive=train.commentPositive.apply(clean)\n",
    "\n",
    "    test.comment=test.comment.apply(clean)\n",
    "    test.commentNegative=test.comment.apply(clean)\n",
    "    test.commentPositive=test.comment.apply(clean)\n",
    "    \n",
    "    tkn=Tokenizer(filters=\"\")\n",
    "    tkn.fit_on_texts(train.comment+train.commentNegative+train.commentPositive)\n",
    "    \n",
    "    comments=tkn.texts_to_sequences(train.comment)\n",
    "    comments_neg=tkn.texts_to_sequences(train.commentNegative)\n",
    "    comments_pos=tkn.texts_to_sequences(train.commentPositive)\n",
    "\n",
    "    t_comments=tkn.texts_to_sequences(test.comment)\n",
    "    t_comments_neg=tkn.texts_to_sequences(test.commentNegative)\n",
    "    t_comments_pos=tkn.texts_to_sequences(test.commentPositive)\n",
    "\n",
    "    c_len=int(np.percentile(list(map(len,comments)),95))\n",
    "    cneg_len=int(np.percentile(list(map(len,comments_neg)),95))\n",
    "    cpos_len=int(np.percentile(list(map(len,comments_pos)),95))\n",
    "\n",
    "    c_pad=pad_sequences(comments,c_len)\n",
    "    cneg_pad=pad_sequences(comments_neg,cneg_len)\n",
    "    cpos_pad=pad_sequences(comments_pos,cpos_len)\n",
    "\n",
    "    t_c_pad=pad_sequences(t_comments,c_len)\n",
    "    t_cneg_pad=pad_sequences(t_comments_neg,cneg_len)\n",
    "    t_cpos_pad=pad_sequences(t_comments_pos,cpos_len)\n",
    "\n",
    "    y=((train.reting.values.astype(np.float32))-1)/4\n",
    "\n",
    "    return c_pad,cneg_pad,cpos_pad,t_c_pad,t_cneg_pad,t_cpos_pad,y\n",
    "\n",
    "def prepare_for_cnn(train,test):\n",
    "    words = 256\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train.comment+train.commentNegative+train.commentPositive)\n",
    "    sequences = tokenizer.texts_to_sequences(train.comment)\n",
    "    word_index = tokenizer.word_index\n",
    "    X_train = pad_sequences(sequences, maxlen = words)\n",
    "\n",
    "    pos_sequences = tokenizer.texts_to_sequences(train.commentPositive)\n",
    "    pX_train = pad_sequences(pos_sequences, maxlen = words)\n",
    "\n",
    "    neg_sequences = tokenizer.texts_to_sequences(train.commentNegative)\n",
    "    nX_train = pad_sequences(neg_sequences, maxlen = words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.comment)\n",
    "    X_test = pad_sequences(s, maxlen=words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.commentPositive)\n",
    "    pX_test = pad_sequences(s, maxlen=words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.commentNegative)\n",
    "    nX_test = pad_sequences(s, maxlen=words)\n",
    "    \n",
    "    y_train=((train.reting.values.astype(np.float32))-1)/4\n",
    "    return X_train,pX_train,nX_train,X_test,pX_test,nX_test,y_train\n",
    "\n",
    "def train_rnn(c_pad,cneg_pad,cpos_pad,y):\n",
    "    n_words=23023\n",
    "    comm=Input((c_pad.shape[1],))\n",
    "    cneg=Input((cneg_pad.shape[1],))\n",
    "    cpos=Input((cpos_pad.shape[1],))\n",
    "\n",
    "    m_comm=Masking()(comm)\n",
    "    m_cneg=Masking()(cneg)\n",
    "    m_cpos=Masking()(cpos)\n",
    "\n",
    "    enc_lstm=Bidirectional(LSTM(256,return_sequences=True,dropout=0.2))\n",
    "    enc2_lstm=LSTM(256,dropout=0.2)\n",
    "\n",
    "    emb=Embedding(n_words,64)\n",
    "\n",
    "    comm_emb=emb(m_comm)\n",
    "    cneg_emb=emb(m_cneg)\n",
    "    cpos_emb=emb(m_cpos)\n",
    "\n",
    "    comm_enc=enc_lstm(comm_emb)\n",
    "    cneg_enc=enc_lstm(cneg_emb)\n",
    "    cpos_enc=enc_lstm(cpos_emb)\n",
    "\n",
    "    comm_enc2=enc2_lstm(comm_enc)\n",
    "    cneg_enc2=enc2_lstm(cneg_enc)\n",
    "    cpos_enc2=enc2_lstm(cpos_enc)\n",
    "\n",
    "    conc=Concatenate()([comm_enc2,cneg_enc2,cpos_enc2])\n",
    "\n",
    "    res=Dense(64,activation=\"selu\")(conc)\n",
    "    res=Dense(1,activation=\"sigmoid\")(res)\n",
    "\n",
    "    model=Model([comm,cneg,cpos],res)\n",
    "    model.compile(\"adam\",\"mse\")\n",
    "    model.fit([c_pad,cneg_pad,cpos_pad],y,batch_size=512,epochs=3)\n",
    "    return model\n",
    "    \n",
    "def train_cnn(X_train,pX_train,nX_train,y_train):\n",
    "    n_words=23109\n",
    "    words = 256\n",
    "    \n",
    "    embedding_layer = Embedding(n_words, 50, input_length=words)\n",
    "\n",
    "    sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    pos_sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    pos_embedded_sequences = embedding_layer(pos_sequence_input)\n",
    "\n",
    "    neg_sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    neg_embedded_sequences = embedding_layer(neg_sequence_input)\n",
    "\n",
    "    x = Concatenate()([embedded_sequences, pos_embedded_sequences, neg_embedded_sequences])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(256, 4, padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 4, padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 4, activation='selu', padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 5, activation='selu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "    x = Dense(64, activation='selu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, pos_sequence_input, neg_sequence_input], outputs=x)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit([X_train, pX_train, nX_train], y_train, epochs=6, batch_size=512)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data:\n",
      "Preparing data for RNN:\n",
      "Preparing data for CNN:\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data:\")\n",
    "train,test=read_and_split(\"X_train.csv\")\n",
    "print(\"Preparing data for RNN:\")\n",
    "m=Mystem()\n",
    "c_pad,cneg_pad,cpos_pad,t_c_pad,t_cneg_pad,t_cpos_pad,y=prepare_for_rnn(train,test)\n",
    "print(\"Preparing data for CNN:\")\n",
    "X_train,pX_train,nX_train,X_test,pX_test,nX_test,y_train=prepare_for_cnn(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN:\n",
      "Epoch 1/3\n",
      "12469/12469 [==============================] - 14s - loss: 0.1200    \n",
      "Epoch 2/3\n",
      "12469/12469 [==============================] - 12s - loss: 0.0754    \n",
      "Epoch 3/3\n",
      "12469/12469 [==============================] - 12s - loss: 0.0426    \n",
      "Training CNN:\n",
      "Epoch 1/6\n",
      "12469/12469 [==============================] - 3s - loss: 0.1649     \n",
      "Epoch 2/6\n",
      "12469/12469 [==============================] - 2s - loss: 0.1145     \n",
      "Epoch 3/6\n",
      "12469/12469 [==============================] - 2s - loss: 0.0839     \n",
      "Epoch 4/6\n",
      "12469/12469 [==============================] - 2s - loss: 0.0521     \n",
      "Epoch 5/6\n",
      "12469/12469 [==============================] - 2s - loss: 0.0339     \n",
      "Epoch 6/6\n",
      "12469/12469 [==============================] - 2s - loss: 0.0253     \n"
     ]
    }
   ],
   "source": [
    "print(\"Training RNN:\")\n",
    "rnn=train_rnn(c_pad,cneg_pad,cpos_pad,y)\n",
    "print(\"Training CNN:\")\n",
    "cnn=train_cnn(X_train,pX_train,nX_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже можно увидеть результат на отложенной выборке по трём основным регрессионным метрикам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.975185072960638\n",
      "Mean absolute error: 0.6467620786754958\n",
      "R^2 score: 0.442469823665124\n"
     ]
    }
   ],
   "source": [
    "pred_rnn=rnn.predict([t_c_pad,t_cneg_pad,t_cpos_pad])*4+1\n",
    "pred_cnn=cnn.predict([X_test,pX_test,nX_test])*4+1\n",
    "final_pred=np.mean([pred_rnn,pred_cnn],axis=0)\n",
    "print(\"Mean squared error: {}\\nMean absolute error: {}\\nR^2 score: {}\".format(mean_squared_error(test.reting,final_pred),\n",
    "                                                                          mean_absolute_error(test.reting,final_pred),\n",
    "                                                                          r2_score(test.reting,final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
