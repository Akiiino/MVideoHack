{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, Embedding, Input, Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Concatenate, Dense, Input, Embedding, Bidirectional, AlphaDropout, Masking, GRU\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_split(filename):\n",
    "    df=pd.read_csv(filename).fillna(\"\")\n",
    "    train,test=train_test_split(df,test_size=0.2)\n",
    "    return train,test\n",
    "\n",
    "def clean(x):\n",
    "    return ''.join(m.lemmatize(re.sub('([^а-яa-z]+)',' ',x.lower()))).strip()\n",
    "\n",
    "def prepare_for_rnn(train,test):\n",
    "    train.comment=train.comment.apply(clean)\n",
    "    train.commentNegative=train.commentNegative.apply(clean)\n",
    "    train.commentPositive=train.commentPositive.apply(clean)\n",
    "\n",
    "    test.comment=test.comment.apply(clean)\n",
    "    test.commentNegative=test.comment.apply(clean)\n",
    "    test.commentPositive=test.comment.apply(clean)\n",
    "    \n",
    "    tkn=Tokenizer(filters=\"\")\n",
    "    tkn.fit_on_texts(train.comment+train.commentNegative+train.commentPositive)\n",
    "    \n",
    "    comments=tkn.texts_to_sequences(train.comment)\n",
    "    comments_neg=tkn.texts_to_sequences(train.commentNegative)\n",
    "    comments_pos=tkn.texts_to_sequences(train.commentPositive)\n",
    "\n",
    "    t_comments=tkn.texts_to_sequences(test.comment)\n",
    "    t_comments_neg=tkn.texts_to_sequences(test.commentNegative)\n",
    "    t_comments_pos=tkn.texts_to_sequences(test.commentPositive)\n",
    "\n",
    "    c_len=int(np.percentile(list(map(len,comments)),95))\n",
    "    cneg_len=int(np.percentile(list(map(len,comments_neg)),95))\n",
    "    cpos_len=int(np.percentile(list(map(len,comments_pos)),95))\n",
    "\n",
    "    c_pad=pad_sequences(comments,c_len)\n",
    "    cneg_pad=pad_sequences(comments_neg,cneg_len)\n",
    "    cpos_pad=pad_sequences(comments_pos,cpos_len)\n",
    "\n",
    "    t_c_pad=pad_sequences(t_comments,c_len)\n",
    "    t_cneg_pad=pad_sequences(t_comments_neg,cneg_len)\n",
    "    t_cpos_pad=pad_sequences(t_comments_pos,cpos_len)\n",
    "\n",
    "    y=((train.reting.values.astype(np.float32))-1)/4\n",
    "\n",
    "    return c_pad,cneg_pad,cpos_pad,t_c_pad,t_cneg_pad,t_cpos_pad,y\n",
    "\n",
    "def prepare_for_cnn(train,test):\n",
    "    words = 256\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train.comment+train.commentNegative+train.commentPositive)\n",
    "    sequences = tokenizer.texts_to_sequences(train.comment)\n",
    "    word_index = tokenizer.word_index\n",
    "    X_train = pad_sequences(sequences, maxlen = words)\n",
    "\n",
    "    pos_sequences = tokenizer.texts_to_sequences(train.commentPositive)\n",
    "    pX_train = pad_sequences(pos_sequences, maxlen = words)\n",
    "\n",
    "    neg_sequences = tokenizer.texts_to_sequences(train.commentNegative)\n",
    "    nX_train = pad_sequences(neg_sequences, maxlen = words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.comment)\n",
    "    X_test = pad_sequences(s, maxlen=words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.commentPositive)\n",
    "    pX_test = pad_sequences(s, maxlen=words)\n",
    "\n",
    "    s = tokenizer.texts_to_sequences(test.commentNegative)\n",
    "    nX_test = pad_sequences(s, maxlen=words)\n",
    "    \n",
    "    y_train=((train.reting.values.astype(np.float32))-1)/4\n",
    "    return X_train,pX_train,nX_train,X_test,pX_test,nX_test,y_train\n",
    "\n",
    "def train_rnn(c_pad,cneg_pad,cpos_pad,y):\n",
    "    n_words=23023\n",
    "    comm=Input((c_pad.shape[1],))\n",
    "    cneg=Input((cneg_pad.shape[1],))\n",
    "    cpos=Input((cpos_pad.shape[1],))\n",
    "\n",
    "    m_comm=Masking()(comm)\n",
    "    m_cneg=Masking()(cneg)\n",
    "    m_cpos=Masking()(cpos)\n",
    "\n",
    "    enc_lstm=Bidirectional(LSTM(256,return_sequences=True,dropout=0.2))\n",
    "    enc2_lstm=LSTM(256,dropout=0.2)\n",
    "\n",
    "    emb=Embedding(n_words,64)\n",
    "\n",
    "    comm_emb=emb(m_comm)\n",
    "    cneg_emb=emb(m_cneg)\n",
    "    cpos_emb=emb(m_cpos)\n",
    "\n",
    "    comm_enc=enc_lstm(comm_emb)\n",
    "    cneg_enc=enc_lstm(cneg_emb)\n",
    "    cpos_enc=enc_lstm(cpos_emb)\n",
    "\n",
    "    comm_enc2=enc2_lstm(comm_enc)\n",
    "    cneg_enc2=enc2_lstm(cneg_enc)\n",
    "    cpos_enc2=enc2_lstm(cpos_enc)\n",
    "\n",
    "    conc=Concatenate()([comm_enc2,cneg_enc2,cpos_enc2])\n",
    "\n",
    "    res=Dense(64,activation=\"selu\")(conc)\n",
    "    res=Dense(1,activation=\"sigmoid\")(res)\n",
    "\n",
    "    model=Model([comm,cneg,cpos],res)\n",
    "    model.compile(\"adam\",\"mse\")\n",
    "    model.fit([c_pad,cneg_pad,cpos_pad],y,batch_size=512,epochs=3)\n",
    "    return model\n",
    "    \n",
    "def train_cnn(X_train,pX_train,nX_train,y_train):\n",
    "    n_words=23109\n",
    "    words = 256\n",
    "    \n",
    "    embedding_layer = Embedding(n_words, 50, input_length=words)\n",
    "\n",
    "    sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    pos_sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    pos_embedded_sequences = embedding_layer(pos_sequence_input)\n",
    "\n",
    "    neg_sequence_input = Input(shape=(words, ), dtype='float32')\n",
    "    neg_embedded_sequences = embedding_layer(neg_sequence_input)\n",
    "\n",
    "    x = Concatenate()([embedded_sequences, pos_embedded_sequences, neg_embedded_sequences])\n",
    "    x = Dropout(0.9)(x)\n",
    "    x = Conv1D(256, 4, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 4, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 4, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Conv1D(256, 4, activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.9)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, pos_sequence_input, neg_sequence_input], outputs=x)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit([X_train, pX_train, nX_train], y_train, epochs=10, batch_size=512)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data:\n",
      "Preparing data for RNN:\n",
      "Preparing data for CNN:\n",
      "Training RNN:\n",
      "Epoch 1/3\n",
      "12469/12469 [==============================] - 14s - loss: 0.1178    \n",
      "Epoch 2/3\n",
      "12469/12469 [==============================] - 12s - loss: 0.0720    \n",
      "Epoch 3/3\n",
      "12469/12469 [==============================] - 12s - loss: 0.0429    \n",
      "Training CNN:\n",
      "Epoch 1/10\n",
      "12469/12469 [==============================] - 5s - loss: 0.1232     \n",
      "Epoch 2/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.1123     \n",
      "Epoch 3/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.1091     \n",
      "Epoch 4/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0928     \n",
      "Epoch 5/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0821     \n",
      "Epoch 6/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0753     \n",
      "Epoch 7/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0720     \n",
      "Epoch 8/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0664     \n",
      "Epoch 9/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0623     \n",
      "Epoch 10/10\n",
      "12469/12469 [==============================] - 2s - loss: 0.0565     \n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data:\")\n",
    "train,test=read_and_split(\"X_train.csv\")\n",
    "print(\"Preparing data for RNN:\")\n",
    "m=Mystem()\n",
    "c_pad,cneg_pad,cpos_pad,t_c_pad,t_cneg_pad,t_cpos_pad,y=prepare_for_rnn(train,test)\n",
    "print(\"Preparing data for CNN:\")\n",
    "X_train,pX_train,nX_train,X_test,pX_test,nX_test,y_train=prepare_for_cnn(train,test)\n",
    "print(\"Training RNN:\")\n",
    "rnn=train_rnn(c_pad,cneg_pad,cpos_pad,y)\n",
    "print(\"Training CNN:\")\n",
    "cnn=train_cnn(X_train,pX_train,nX_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rnn=rnn.predict([t_c_pad,t_cneg_pad,t_cpos_pad])*4+1\n",
    "pred_cnn=cnn.predict([X_test,pX_test,nX_test])*4+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.0420059276390916\n",
      "Mean absolute error: 0.9345256601640092\n",
      "R^2 score: 0.21161647416071705\n"
     ]
    }
   ],
   "source": [
    "final_pred=np.mean([pred_rnn,pred_cnn],axis=0)\n",
    "print(\"Mean squared error: {}\\nMean absolute error: {}\\nR^2 score: {}\".format(mean_squared_error(test.reting,pred_rnn),\n",
    "                                                                          mean_absolute_error(test.reting,final_pred),\n",
    "                                                                          r2_score(test.reting,final_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
